---
title: "Regularized Regression"
---

In linear regression, regularization is a process of making the model more regular or simpler by shrinking the model coefficient to be closer to zero or absolute, ultimately to address over fitting. 

The flow to conduct regularized regression:

1. Split Data: train - validate - test

2. Correlation plot on training data

3. Fit the model on training data

4. Choose the best lambda from the validation test

5. Evaluate the model on test data using MAE, MAPE, and RMSE


We need to load the data set first
```{r}
library(readr)
boston <- read_csv("C:/Users/SU - NB0130/Desktop/bootcamp data science/week 9/day 20/dibimbing-materials-main/boston.csv")
```
The data is about predicting housing price (medv) in Boston city, features:

-   Criminal rate (crim)

-   Residential land zoned proportion (zn)

-   Non-retail business acres proportion (indus)

-   Is bounds with river (chas)

-   Nitrogen oxides concentration (nox)

-   Number rooms average (rm)

-   Owner age proportion (age)

-   Weighted distance to cities (dis)

-   Accessibility index (rad)

-   Tax rate (tax)

-   Pupil-teacher ratio (ptratio)

-   Black proportion (black)

-   Percent lower status (lstat)

### 1. Split the data into train, validate, and test data
**Pre-training and testing with comparison 80:20**
```{r}
library(caTools)
set.seed(123)
sample <- sample.split(boston$medv, SplitRatio = .80)
pre_train <- subset(boston, sample == TRUE)
sample_train <- sample.split(pre_train$medv, SplitRatio = .80)
```

**After that, split pre-train data into train and validation (80:20)**
```{r}
train <- subset(pre_train, sample_train == TRUE)
validation <- subset(pre_train, sample_train == FALSE)
```

**Test data**
```{r}
test <- subset(boston, sample == FALSE)
```

### 2. Draw a pair plot to obtain a feature correlation and prevent multicolinearity
```{r}
library(psych)
pairs.panels(train, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE, # show density plots
             ellipses = TRUE) # show correlation ellipses
```

With 0.8 as the threshold, the correlated features are rad and tax. Therefore, we need to drop one of these columns.

The absolute value of correlation for rad and medv is 0.44

The absolute value of correlation for tax and medv is 0.53

From these 2 values, tax has highest correlation towards medv. Therefore, we keep tax column for our model and drop rad column.


**Drop correlated column**
```{r}
library(dplyr)
drop_cols <- ("rad")

train <- train %>% select(-drop_cols)
validation <-  validation %>% select(-drop_cols)
test <- test %>% select(-drop_cols)
```

### 3. Train multiple models with different lambdas 

We need to do pre processing to ensure we handle categorical features (transform categorical into numeric column).
```{r}
x <- model.matrix(medv ~ ., train)[,-1]
y <-  train$medv
```

Next, we can use two methods of regularized in linear regressions: Ridge and LASSO regressions

**Ridge**

```{r}
library(glmnet)
```
Ridge regression with lambda = 0.01
```{r}
ridge_reg_pointzeroone <- glmnet(x, y, alpha = 0, lambda = 0.01)
coef(ridge_reg_pointzeroone)
```

Ridge regression with lambda = 0.1
```{r}
ridge_reg_pointone <- glmnet(x, y, alpha = 0, lambda = 0.1)
coef(ridge_reg_pointone)
```

Ridge regression with lambda = 1
```{r}
ridge_reg_one <- glmnet(x, y, alpha = 0, lambda = 1)
coef(ridge_reg_pointone)
```

Ridge regression with lambda = 10
```{r}
ridge_reg_ten <- glmnet(x, y, alpha = 0, lambda = 10)
coef(ridge_reg_ten)
```

**LASSO**

LASSO regression with lambda = 0.01
```{r}
lasso_reg_pointzeroone <- glmnet(x, y, alpha = 1, lambda = 0.01)
coef(lasso_reg_pointzeroone) 
```

LASSO regression with lambda = 0.1
```{r}
lasso_reg_pointone <- glmnet(x, y, alpha = 1, lambda = 0.1)
coef(lasso_reg_pointone) 
```

LASSO regression with lambda = 1
```{r}
lasso_reg_one <- glmnet(x, y, alpha = 1, lambda = 1)
coef(lasso_reg_pointone)
```

LASSO regression with lambda = 10
```{r}
lasso_reg_ten <- glmnet(x, y, alpha = 1, lambda = 10)
coef(lasso_reg_ten)
```

### 4. Choose the best lambda from the validation set
```{r}
x_validation <- model.matrix(medv ~., validation)[,-1]
y_validation <- validation$medv
```

**Ridge**

RMSE Ridge for lambda = 0.01
```{r}
RMSE_ridge_pointzeroone <- sqrt(mean((y_validation - predict(ridge_reg_pointzeroone, x_validation))^2))
RMSE_ridge_pointzeroone
```
RMSE Ridge for lambda = 0.1
```{r}
RMSE_ridge_pointone <- sqrt(mean((y_validation - predict(ridge_reg_pointone, x_validation))^2))
RMSE_ridge_pointone
```
RMSE Ridge for lambda = 1
```{r}
RMSE_ridge_one <- sqrt(mean((y_validation - predict(ridge_reg_one, x_validation))^2))
RMSE_ridge_one
```
RMSE Ridge for lambda = 10
```{r}
RMSE_ridge_ten <- sqrt(mean((y_validation - predict(ridge_reg_ten, x_validation))^2))
RMSE_ridge_ten
```
To have the best model's coefficients, we need to find the smallest RMSE on validation data. Hence, the best lambda is 0.01
```{r}
coef(ridge_reg_pointzeroone)
```
This implies that the best model for Ridge regression is:

medv = 2.807966e+01 -7.972347e-02 crim + 3.796482e-02 zn -4.106178e-02 indus + 2.893259e+00 chas -1.602703e+01 nox + 4.517287e+00 rm + 5.679736e-03 age -1.314253e+00 dis -2.421124e-04 tax -9.031044e-01 ptratio + 6.572154e-03 black -4.779743e-01 lstat

For example, an increase of 1 point in zn, while the other features are kept fixed, is associated with an increase of 3.796482e-02 point in medv.

**LASSO**

RMSE LASSO for lambda = 0.01
```{r}
RMSE_lasso_pointzeroone <- sqrt(mean((y_validation - predict(lasso_reg_pointzeroone, x_validation))^2))
RMSE_lasso_pointzeroone
```
RMSE LASSO for lambda = 0.1
```{r}
RMSE_lasso_pointone <- sqrt(mean((y_validation - predict(lasso_reg_pointone, x_validation))^2))
RMSE_lasso_pointone
```
RMSE LASSO for lambda = 1
```{r}
RMSE_lasso_one <- sqrt(mean((y_validation - predict(lasso_reg_one, x_validation))^2))
RMSE_lasso_one
```
RMSE LASSO for lambda = 10
```{r}
RMSE_lasso_ten <- sqrt(mean((y_validation - predict(lasso_reg_ten, x_validation))^2))
RMSE_lasso_ten
```
Best model's coefficients of LASSO regression is when the lambda is 0.01 (its RMSE is the smallest)
```{r}
coef(lasso_reg_pointzeroone)
```
This implies that:

medv = 2.782960e+01 -7.879101e-02 crim + 3.673332e-02 zn -3.849552e-02 indus + 2.864983e+00 chas -1.574647e+01 nox + 4.531757e+00 rm + 4.411184e-03 age -1.294476e+00 dis -2.439776e-04 tax -9.039250e-01 ptratio + 6.556538e-03 black -4.764532e-01 lstat 

For instance, an increase of 1 point in zn, while the other features are kept fixed, is associated with an increase of 3.673332e-02 point in medv.


### 5. Evaluate the best models on the test data 
```{r}
x_test <- model.matrix(medv ~ ., test)[,-1]
y_test <-  test$medv
```

**Ridge**

##RMSE
```{r}
RMSE_ridge_best <- sqrt(mean((y_test - predict(ridge_reg_pointzeroone, x_test))^2))
RMSE_ridge_best
```
The standard deviation of prediction errors to the regression line is 6.820639

##MAE
```{r}
MAE_ridge_best <- mean(abs(y_test-predict(ridge_reg_pointzeroone, x_test)))
MAE_ridge_best
```
Our prediction deviates from the actual data (the true housing price (medv)) as much as 3.896186

##MAPE
```{r}
MAPE_ridge_best <- mean(abs((predict(ridge_reg_pointzeroone, x_test) - y_test))/y_test)
MAPE_ridge_best
```
MAE value of 3.896186 is equivalent to 17.1% deviation relative to the true housing price

**LASSO**

##RMSE
```{r}
RMSE_ridge_best <- sqrt(mean((y_test - predict(lasso_reg_pointzeroone, x_test))^2))
RMSE_ridge_best
```
The standard deviation of prediction errors to the regression line is 6.823445

##MAE
```{r}
MAE_ridge_best <- mean(abs(y_test-predict(lasso_reg_pointzeroone, x_test)))
MAE_ridge_best
```
Our prediction deviates from the the true housing price as much as 3.888415

##MAPE
```{r}
MAPE_ridge_best <- mean(abs((predict(lasso_reg_pointzeroone, x_test) - y_test))/y_test)
MAPE_ridge_best
```
3.888415 in MAE is equivalent to 17.07% deviation relative to the true housing price